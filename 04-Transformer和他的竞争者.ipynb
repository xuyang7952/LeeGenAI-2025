{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8186879",
   "metadata": {},
   "source": [
    "# 序列建模前沿技术综述（2025）\n",
    "\n",
    "## 1. RNN（循环神经网络）\n",
    "### 核心思想\n",
    "RNN通过时间步的递归结构处理序列数据，利用隐藏状态 $h_t$ 捕捉历史信息\n",
    "\n",
    "$$h_t = \\sigma(W_h h_{t-1} + W_x x_t)$$\n",
    "\n",
    "其中：\n",
    "- $h_t$: 当前隐藏状态\n",
    "- $x_t$: 时间步$t$的输入\n",
    "- $\\sigma$: 激活函数（如Tanh/ReLU）\n",
    "\n",
    "### 问题与改进\n",
    "**梯度问题**：\n",
    "- 梯度消失/爆炸 RNN在长序列中难以学习长期依赖。\n",
    "- （LSTM/GRU解决）引入门控机制（输入门、遗忘门、输出门），控制信息流动：\n",
    "  \n",
    "**LSTM门控机制**：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_t &= \\sigma(W_f [h_{t-1}, x_t]) \\\\\n",
    "i_t &= \\sigma(W_i [h_{t-1}, x_t]) \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tanh(W_C [h_{t-1}, x_t]) \\\\\n",
    "o_t &= \\sigma(W_o [h_{t-1}, x_t]) \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f203f3",
   "metadata": {},
   "source": [
    "## 2. Transformer\n",
    "\n",
    "Transformer通过 自注意力机制（Self-Attention）替代RNN的递归结构，直接建模全局依赖：\n",
    "\n",
    "### 自注意力机制\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $Q = XW^Q$, $K = XW^K$, $V = XW^V$\n",
    "- $d_k$: 缩放因子（防止点积过大）\n",
    "\n",
    "### 多头注意力\n",
    "\n",
    "将输入拆分为多个头（Head），并行计算不同子空间的信息：\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O\n",
    "$$\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "### 位置编码\n",
    "\n",
    "通过正弦/余弦函数或学习嵌入向量注入序列位置信息：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "PE_{(pos,2i)} &= \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right) \\\\\n",
    "PE_{(pos,2i+1)} &= \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 优势\n",
    "\n",
    "- 并行化：适合GPU/TPU加速。\n",
    "- 长距离依赖：直接建模全局关系。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8845513",
   "metadata": {},
   "source": [
    "## 3. Attention机制演进\n",
    "### 传统Attention vs. Self-Attention\n",
    "- 传统Attention：在序列到序列模型中，通过注意力权重对齐源序列和目标序列。\n",
    "- Self-Attention：序列内部元素间的相关性建模（如Transformer）。\n",
    "\n",
    "### 线性注意力\n",
    "为解决自注意力的高计算复杂度O(n^2)，提出线性注意力：\n",
    "\n",
    "$$\n",
    "\\text{LinearAttention}(Q,K,V) = \\phi(Q)^T \\phi(K)V\n",
    "$$\n",
    "- 通过核函数$\\phi$将点积转换为低维空间操作降低计算复杂度至$O(n)$\n",
    "\n",
    "### 改进\n",
    "- RetNet：引入 Retention Mechanism，通过反射（Reflection）机制逐步遗忘旧信息：\n",
    "$$\n",
    "h t​=α t​⋅h t−1​+(1−α t​)⋅LinearAttention(x t​)\n",
    "$$\n",
    "- Gated Retention：增加门控控制遗忘速率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55379d14",
   "metadata": {},
   "source": [
    "## 4. RNN改进方案\n",
    "### DeltaNet\n",
    "\n",
    "将RNN与线性注意力结合，通过差分方程建模序列：\n",
    "\n",
    "$$\n",
    "h_t = h_{t-1} + \\Delta h_t, \\quad \\Delta h_t = \\text{LinearAttention}(x_t)\n",
    "$$\n",
    "\n",
    "利用线性注意力的低复杂度特性，提升RNN的效率。\n",
    "\n",
    "### Linger（SSM模型）\n",
    "\n",
    "通过 状态空间模型（SSM） 改进RNN，捕捉长序列依赖：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= A h_{t-1} + B x_t \\\\\n",
    "y_t &= C h_t + D x_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中 A,B,C,D 是可学习参数，SSM的计算复杂度为 O(n)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dfe303",
   "metadata": {},
   "source": [
    "## 5. Mamba（选择性SSM）\n",
    "\n",
    "核心思想\n",
    "Mamba通过 选择性状态空间模型（Selective SSM） 处理长序列，结合线性注意力和RNN的优势：\n",
    "\n",
    "### 核心方程\n",
    "$$\n",
    "\\Delta h_t = A(\\theta) \\cdot h_{t-1} + B(\\theta) \\cdot x_t\n",
    "$$\n",
    "- $A(\\theta), B(\\theta)$: 输入依赖的动态参数\n",
    "\n",
    "是输入依赖的可学习参数，通过门控选择性更新状态。\n",
    "\n",
    "### 关键创新\n",
    "- 选择性机制：根据输入动态调整状态转移矩阵，提升灵活性。\n",
    "- 高效计算：利用线性代数优化，将复杂度从 $O(n^2)$ 降至 $O(n)$\n",
    "- 对比Transformer\n",
    "    - 长序列优势：Mamba在超长序列（如100k tokens）上显著优于Transformer。\n",
    "    - 硬件适配性：更适合GPU/TPU的并行计算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1546515a",
   "metadata": {},
   "source": [
    "## 6. 最新研究趋势\n",
    "| 方向              | 代表技术               | 核心改进                 |\n",
    "|-------------------|------------------------|--------------------------|\n",
    "| 模型压缩         | LoLCATs              | 低秩注意力权重迁移       |\n",
    "| 记忆增强         | Titans               | 动态记忆参数学习         |\n",
    "| Mamba扩展        | MambaOut            | 视觉任务适配             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47968d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c5c796",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
